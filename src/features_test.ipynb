{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Behavioral Phenotyping with 3D Skeletal Pose\n",
    "Joshua Wu\n",
    "\n",
    "Timothy Dunn Lab\n",
    "\n",
    "14 October 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neurodegenerative diseases (like Parkinson's) are characterized by a wide variety of behavioral defects or movement deficits. However, behavior and movement have historically been difficult to quantify and measure.\n",
    "\n",
    "AIMS (for LID)\n",
    "\n",
    "<img src='../../CAPTURE_demo/Python_analysis/engine/demo/no_updrs.png' width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now with advances in 3D vision - we have DANNCE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](../../CAPTURE_demo/Python_analysis/engine/demo/DANNCE_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, it is intuitive that we need these types of measurements to objectively quantify behavior. But it is unclear how we leverage this data to actually make scientific discoveries.\n",
    "\n",
    "**GOAL**: Create robust and interpretable un-/self-/semi- supervised analysis frameworks downstream of 3D vision measurements for objectively phenotyping and quantifying behavior.\n",
    "\n",
    "1. Species-agnostic\n",
    "2. Batch-effect tolerant\n",
    "3. Dimension tolerant\n",
    "4. Statistically validated\n",
    "5. Interpretable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, there was the CAPTURE MATLAB package (Marshall 2020), which was built off of the MotionMapper (Berman 2014) pipeline for 3D motion capture. It was later adapted for DANNCE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://github.com/jessedmarshall/CAPTURE_demo/raw/master/Common/demo_figure.png width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook represents a faster and more memory efficient Python package for the analysis of behavioral data, which can interface with future frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "import this\n",
    "from features import *\n",
    "import DataStruct as ds\n",
    "import visualization as vis\n",
    "import interface as itf\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import Video\n",
    "from embed import Watershed\n",
    "import copy\n",
    "from typing import Optional, Union, List\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pose predictions and keypoint connectivity information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected older version of '.mat' file\n",
      "Snout\n",
      "EarR\n",
      "EarL\n",
      "SpineF\n",
      "SpineM\n",
      "Tail_base_\n",
      "Forepaw_R\n",
      "Wrist_R\n",
      "ForeLimb_R\n",
      "Forepaw_L\n",
      "Wrist_L\n",
      "Forelimb_L\n",
      "Hindpaw_R\n",
      "Ankel_R\n",
      "Hindlimb_R\n",
      "Hindpaw_L\n",
      "Ankel_L\n",
      "Hindlimb_L\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<DataStruct.DataStruct at 0x7fde4f1a0ac0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pstruct = ds.DataStruct(config_path = '../configs/embedding_analysis_ws_r01.yaml')\n",
    "pstruct.load_connectivity()\n",
    "pstruct.load_pose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some skeletons together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:16<00:00, 11.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis.skeleton_vid3D(pstruct.pose_3d,\n",
    "                   pstruct.connectivity,\n",
    "                   frames=[1000,500000,2000000],\n",
    "                   N_FRAMES = 200,\n",
    "                   dpi = 100,\n",
    "                   VID_NAME='vid_raw.mp4',\n",
    "                   SAVE_ROOT=pstruct.out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"/home/exx/Desktop/GitHub/results/R01_all/vis_vid_raw.mp4\" controls  width=\"600\"  height=\"600\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(pstruct.out_path + 'vis_vid_raw.mp4', width=600, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median filter and align floor across videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting and rotating the floor for each video to alignment ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:06<00:00,  1.11it/s]\n",
      "100%|██████████| 200/200 [00:17<00:00, 11.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate videos have rotated floor planes - this rotates them back\n",
    "# Also contains a median filter\n",
    "pstruct.pose_3d = align_floor(pstruct.pose_3d, \n",
    "                              pstruct.exp_ids_full) # median filter size\n",
    "\n",
    "# Check alignment\n",
    "vis.skeleton_vid3D(pstruct.pose_3d,\n",
    "                   pstruct.connectivity,\n",
    "                   frames=[1000,500000,2000000],\n",
    "                   N_FRAMES = 200,\n",
    "                   dpi = 100,\n",
    "                   VID_NAME='vid_aligned.mp4',\n",
    "                   SAVE_ROOT=pstruct.out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"/home/exx/Desktop/GitHub/results/R01_all/vis_vid_aligned.mp4\" controls  width=\"600\"  height=\"600\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(pstruct.out_path + 'vis_vid_aligned.mp4', width=600, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the velocity of the Snout, Mid Spine and Tail Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating absolute velocities ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:11<00:00,  1.64s/it]\n"
     ]
    }
   ],
   "source": [
    "# Calculating velocities and standard deviation of velocites over windows\n",
    "abs_vel, abs_vel_labels  = get_velocities(pstruct.pose_3d, \n",
    "                                          pstruct.exp_ids_full, \n",
    "                                          pstruct.connectivity.joint_names,\n",
    "                                          joints=[0,4,5],\n",
    "                                          widths=[5,11,51])\n",
    "\n",
    "# Calculate velocities t+10 - t instead of t+1-t\n",
    "# Should it be an abs val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This includes the vector magnitude, x, y, and z velocities as well as the standard deviations across a sliding window of varying length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abs_vel_vec_Snout_5', 'abs_vel_x_Snout_5', 'abs_vel_y_Snout_5', 'abs_vel_z_Snout_5', 'abs_vel_vec_Snout_11', 'abs_vel_x_Snout_11', 'abs_vel_y_Snout_11', 'abs_vel_z_Snout_11', 'abs_vel_vec_Snout_51', 'abs_vel_x_Snout_51', 'abs_vel_y_Snout_51', 'abs_vel_z_Snout_51', 'abs_vel_std_vec_Snout_5', 'abs_vel_std_x_Snout_5', 'abs_vel_std_y_Snout_5', 'abs_vel_std_z_Snout_5', 'abs_vel_std_vec_Snout_11', 'abs_vel_std_x_Snout_11', 'abs_vel_std_y_Snout_11', 'abs_vel_std_z_Snout_11', 'abs_vel_std_vec_Snout_51', 'abs_vel_std_x_Snout_51', 'abs_vel_std_y_Snout_51', 'abs_vel_std_z_Snout_51']\n"
     ]
    }
   ],
   "source": [
    "print([feat for feat in abs_vel_labels if 'Snout' in feat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching the x-velocity of the Snout with video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snout_vel = abs_vel[:,abs_vel_labels.index('abs_vel_x_Snout_5')]\n",
    "vis.skeleton_vid3D_features(pstruct.pose_3d,\n",
    "                            snout_vel,\n",
    "                            pstruct.connectivity,\n",
    "                            frames=[500000],\n",
    "                            N_FRAMES = 200,\n",
    "                            dpi = 100,\n",
    "                            VID_NAME='Snout_abs_vel.mp4',\n",
    "                            SAVE_ROOT=pstruct.out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"/home/exx/Desktop/GitHub/results/R01_all/vis_feat_Snout_abs_vel.mp4\" controls  width=\"1200\"  height=\"600\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(pstruct.out_path + 'vis_feat_Snout_abs_vel.mp4', width=1200, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can probably very easily find a frame with bad tracking by looking at the frame with the highest absolute velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad Tracking Frame Number 531233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:22<00:00,  9.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mid_vel = abs_vel[:,abs_vel_labels.index('abs_vel_vec_SpineM_5')]\n",
    "max_vel_frame = np.argmax(mid_vel)\n",
    "print(\"Bad Tracking Frame Number \" + str(max_vel_frame))\n",
    "vis.skeleton_vid3D_features(pstruct.pose_3d,\n",
    "                            mid_vel,\n",
    "                            pstruct.connectivity,\n",
    "                            frames=[max_vel_frame],\n",
    "                            N_FRAMES = 200,\n",
    "                            dpi = 100,\n",
    "                            VID_NAME='Snout_abs_vel.mp4',\n",
    "                            SAVE_ROOT=pstruct.out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"/home/exx/Desktop/GitHub/results/R01_all/vis_feat_Snout_abs_vel.mp4\" controls  width=\"1200\"  height=\"600\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(pstruct.out_path + 'vis_feat_Snout_abs_vel.mp4', width=1200, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All other features will be egocentric so we will center and lock the front spine onto the x-z axis by rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centering poses to mid spine ...\n",
      "Rotating spine to xz plane ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:11<00:00, 16.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Centering all joint locations to mid-spine\n",
    "pose_locked = center_spine(pstruct.pose_3d)\n",
    "\n",
    "# Rotates front spine to xz axis\n",
    "pose_locked = rotate_spine(pose_locked)\n",
    "\n",
    "vis.skeleton_vid3D(pose_locked,\n",
    "                   pstruct.connectivity,\n",
    "                   frames=[50000],\n",
    "                   N_FRAMES = 200,\n",
    "                   dpi = 100,\n",
    "                   VID_NAME='vid_centered.mp4',\n",
    "                   SAVE_ROOT=pstruct.out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"/home/exx/Desktop/GitHub/results/R01_all/vis_vid_centered.mp4\" controls  width=\"600\"  height=\"600\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(pstruct.out_path + 'vis_vid_centered.mp4', width=600, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this centered and spine-locked pose transformation, we can calculate relative velocities of all keypoints. We leave out the mid spine since it is zeroed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected centered pose input - calculating relative velocities ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:40<00:00,  5.81s/it]\n"
     ]
    }
   ],
   "source": [
    "# Getting relative velocities\n",
    "rel_vel, rel_vel_labels = get_velocities(pose_locked,\n",
    "                                         pstruct.exp_ids_full, \n",
    "                                         pstruct.connectivity.joint_names,\n",
    "                                         joints=np.delete(np.arange(18),4),\n",
    "                                         widths=[5,11,51])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as before, this includes the vector magnitude, x, y, and z velocities as well as the standard deviations across a sliding window of varying length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rel_vel_vec_Snout_5', 'rel_vel_x_Snout_5', 'rel_vel_y_Snout_5', 'rel_vel_z_Snout_5', 'rel_vel_vec_Snout_11', 'rel_vel_x_Snout_11', 'rel_vel_y_Snout_11', 'rel_vel_z_Snout_11', 'rel_vel_vec_Snout_51', 'rel_vel_x_Snout_51', 'rel_vel_y_Snout_51', 'rel_vel_z_Snout_51', 'rel_vel_std_vec_Snout_5', 'rel_vel_std_x_Snout_5', 'rel_vel_std_y_Snout_5', 'rel_vel_std_z_Snout_5', 'rel_vel_std_vec_Snout_11', 'rel_vel_std_x_Snout_11', 'rel_vel_std_y_Snout_11', 'rel_vel_std_z_Snout_11', 'rel_vel_std_vec_Snout_51', 'rel_vel_std_x_Snout_51', 'rel_vel_std_y_Snout_51', 'rel_vel_std_z_Snout_51']\n"
     ]
    }
   ],
   "source": [
    "print([feat for feat in rel_vel_labels if 'Snout' in feat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's match relative x-velocity of the snout to a video of the skeleton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:19<00:00, 10.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snout_vel = rel_vel[:,rel_vel_labels.index('rel_vel_x_Snout_5')]\n",
    "vis.skeleton_vid3D_features(pose_locked,\n",
    "                            snout_vel,\n",
    "                            pstruct.connectivity,\n",
    "                            frames=[500000],\n",
    "                            N_FRAMES = 200,\n",
    "                            dpi = 100,\n",
    "                            VID_NAME='Snout_rel_vel.mp4',\n",
    "                            SAVE_ROOT=pstruct.out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"/home/exx/Desktop/GitHub/results/R01_all/vis_feat_Snout_rel_vel.mp4\" controls  width=\"1200\"  height=\"600\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(pstruct.out_path + 'vis_feat_Snout_rel_vel.mp4', width=1200, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the relative z-velocity of the snout during a rearing behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:19<00:00, 10.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snout_vel_rear = rel_vel[:,rel_vel_labels.index('rel_vel_z_Snout_5')]\n",
    "vis.skeleton_vid3D_features(pose_locked,\n",
    "                            snout_vel_rear,\n",
    "                            pstruct.connectivity,\n",
    "                            frames=[659600],\n",
    "                            N_FRAMES = 200,\n",
    "                            dpi = 100,\n",
    "                            VID_NAME='Snout_rel_vel_rear.mp4',\n",
    "                            SAVE_ROOT=pstruct.out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"/home/exx/Desktop/GitHub/results/R01_all/vis_feat_Snout_rel_vel_rear.mp4\" controls  width=\"1200\"  height=\"600\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(pstruct.out_path + 'vis_feat_Snout_rel_vel_rear.mp4', width=1200, height=600)\n",
    "\n",
    "# Double check why isnt' this signed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we calculate joint angles.\n",
    "\n",
    "Hopefully, informative joint angles are preselected in `skeletons.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating joint angles ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:09<00:00,  2.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# Calculating joint angles\n",
    "angles, angle_labels = get_angles(pose_locked,\n",
    "                                  pstruct.connectivity.angles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each joint angle is represented by 3 numbers corresponding to the projections of that angle onto each of the 3 axis planes (x-y, x-z, y-z)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ang_3_4_5_xy', 'ang_3_4_5_xz', 'ang_3_4_5_yz']\n"
     ]
    }
   ],
   "source": [
    "# 3 Angles for SpineF -> SpineM -> Tail Base\n",
    "print([label for label in angle_labels if '3_4_5' in label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to that rearing behavior from before look at the xz angle of the back (SpineF -> SpineM -> Tail Base)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:19<00:00, 10.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "back_ang_rear = angles[:,angle_labels.index('ang_3_4_5_xz')]\n",
    "vis.skeleton_vid3D_features(pose_locked,\n",
    "                            back_ang_rear,\n",
    "                            pstruct.connectivity,\n",
    "                            frames=[659600],\n",
    "                            N_FRAMES = 200,\n",
    "                            dpi = 100,\n",
    "                            VID_NAME='back_ang_rear.mp4',\n",
    "                            SAVE_ROOT=pstruct.out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"/home/exx/Desktop/GitHub/results/R01_all/vis_feat_back_ang_rear.mp4\" controls  width=\"1200\"  height=\"600\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(pstruct.out_path + 'vis_feat_back_ang_rear.mp4', width=1200, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Angles calculated are unsigned and from 0 to $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.141591913615472\n",
      "5.906200761379809e-07\n"
     ]
    }
   ],
   "source": [
    "# Discuss more (circular diff in python)\n",
    "print(back_ang_rear.max())\n",
    "print(back_ang_rear.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the frames with the largest angles and the lowest angles to make sure there are no non-continuous jumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis.skeleton_vid3D_features(pose_locked,\n",
    "                            back_ang_rear,\n",
    "                            pstruct.connectivity,\n",
    "                            frames=[np.argmax(back_ang_rear)],\n",
    "                            N_FRAMES = 200,\n",
    "                            dpi = 100,\n",
    "                            VID_NAME='back_ang_rear_max.mp4',\n",
    "                            SAVE_ROOT=pstruct.out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"/home/exx/Desktop/GitHub/results/R01_all/vis_feat_back_ang_rear_max.mp4\" controls  width=\"1200\"  height=\"600\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(pstruct.out_path + 'vis_feat_back_ang_rear_max.mp4', width=1200, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.skeleton_vid3D_features(pose_locked,\n",
    "                            back_ang_rear,\n",
    "                            pstruct.connectivity,\n",
    "                            frames=[np.argmin(back_ang_rear)],\n",
    "                            N_FRAMES = 200,\n",
    "                            dpi = 100,\n",
    "                            VID_NAME='back_ang_rear_min.mp4',\n",
    "                            SAVE_ROOT=pstruct.out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(pstruct.out_path + 'vis_feat_back_ang_rear_min.mp4', width=1200, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these angles we just obtained, we can now calculate angular velocities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating angle velocities\n",
    "ang_vel, ang_vel_labels = get_angular_vel(angles,\n",
    "                                          angle_labels,\n",
    "                                          pstruct.exp_ids_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we did with positional velocities, we also calculate angular velocities and standard deviations over moving windows of different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Angles for SpineF -> SpineM -> Tail Base\n",
    "print([label for label in ang_vel_labels if '3_4_5' in label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to that rearing behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_ang_rear_vel = ang_vel[:,ang_vel_labels.index('avel_3_4_5_xz_5')]\n",
    "vis.skeleton_vid3D_features(pose_locked,\n",
    "                            back_ang_rear_vel,\n",
    "                            pstruct.connectivity,\n",
    "                            frames=[659600],\n",
    "                            N_FRAMES = 200,\n",
    "                            dpi = 100,\n",
    "                            VID_NAME='back_ang_rear_vel.mp4',\n",
    "                            SAVE_ROOT=pstruct.out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(pstruct.out_path + 'vis_feat_back_ang_rear_vel.mp4', width=1200, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also be able to isolate bad tracking with angular velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.skeleton_vid3D_features(pstruct.pose_3d,\n",
    "                            back_ang_rear_vel,\n",
    "                            pstruct.connectivity,\n",
    "                            frames=[np.argmax(back_ang_rear_vel)],\n",
    "                            N_FRAMES = 200,\n",
    "                            dpi = 100,\n",
    "                            VID_NAME='back_ang_rear_vel_max.mp4',\n",
    "                            SAVE_ROOT=pstruct.out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(pstruct.out_path + 'vis_feat_back_ang_rear_vel_max.mp4', width=1200, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are going to save the egocentric x, y, z coordinates as its own set of features\n",
    "\n",
    "This code does not calculate anything - it just reshapes the pose and generates labels for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape pose to get egocentric pose features\n",
    "ego_pose, ego_pose_labels  = get_ego_pose(pose_locked,\n",
    "                                          pstruct.connectivity.joint_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([feat for feat in ego_pose_labels if 'Snout' in feat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just make sure the labels sync up with the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snout_z = ego_pose[:,ego_pose_labels.index('ego_euc_Snout_z')]\n",
    "vis.skeleton_vid3D_features(pose_locked,\n",
    "                            snout_z,\n",
    "                            pstruct.connectivity,\n",
    "                            frames=[659600],\n",
    "                            N_FRAMES = 200,\n",
    "                            dpi = 100,\n",
    "                            VID_NAME='ego_snout_z.mp4',\n",
    "                            SAVE_ROOT=pstruct.out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(pstruct.out_path + 'vis_feat_ego_snout_z.mp4', width=1200, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge features together and clear some memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all features together\n",
    "features = np.concatenate([abs_vel, rel_vel, ego_pose, angles, ang_vel], axis=1)\n",
    "labels = abs_vel_labels + rel_vel_labels + ego_pose_labels + angle_labels + ang_vel_labels\n",
    "\n",
    "# Clear memory\n",
    "del pose_locked, abs_vel, rel_vel, ego_pose, angles, ang_vel\n",
    "del abs_vel_labels, rel_vel_labels, ego_pose_labels, angle_labels, ang_vel_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save or read kinematic/wavelet features from h5 file\n",
    "save_h5(features, labels, path = ''.join([pstruct.out_path,'postural_feats.h5']))\n",
    "# features, labels =  read_h5(path = ''.join([pstruct.out_path,'postural_feats.h5']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time for principal component analysis (PCA). PCA is a dimensionality reduction technique which generates orthogonal axes of high variance upon which to project our data. There are many implementations of PCA, but we will use Facebook's Fast Randomized PCA package (`fbpca`), which is significantly faster than most other implementations.\n",
    "\n",
    "We calculate PCA separately on each feature category to preserve variance and balance the categories. This is in lieu of z-transforming (mean-centering and unit variance) every feature. ** Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "pc_feats, pc_labels = pca(features,\n",
    "                          labels,\n",
    "                          categories = ['abs_vel','rel_vel','ego_euc','ang','avel'],\n",
    "                          n_pcs = 8,\n",
    "                          method = 'fbpca')\n",
    "print(\"PCA time: \" + str(time.time() - t))\n",
    "\n",
    "del features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can observe clustering among the PCs of any of these feature categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_pcs(pc_feats: np.ndarray,\n",
    "                pc_labels: List,\n",
    "                category: str,\n",
    "                pstruct: ds.DataStruct,\n",
    "                pcs = np.s_[:2]):\n",
    "    ''' \n",
    "        Makes density plots of the top 2 PCs from a category of features\n",
    "        Shows the density separately for each condition\n",
    "    '''\n",
    "    # Slicing out top PCs from selected category\n",
    "    cat_pc_idx = [i for i,label in enumerate(pc_labels) if category in label]\n",
    "    cat_pcs = pc_feats[:, cat_pc_idx[pcs]] # First two pcs\n",
    "\n",
    "    # Setting up datastruct\n",
    "    struct = copy.deepcopy(pstruct)\n",
    "    struct.frame_id = np.arange(0,cat_pcs.shape[0],10)\n",
    "    struct.embed_vals = cat_pcs[::10]\n",
    "    struct.exp_id = pstruct.exp_ids_full[::10]\n",
    "    struct.load_meta()\n",
    "\n",
    "    # Watershed clustering, also calculates densities\n",
    "    ws = Watershed(sigma = 15,\n",
    "                max_clip=1,\n",
    "                log_out = True,\n",
    "                pad_factor = 0.05)\n",
    "    struct.data.loc[:, 'Cluster'] = ws.fit_predict(data = struct.embed_vals)\n",
    "\n",
    "    # Plot densities\n",
    "    vis.density_cat(data=struct, column='Condition', watershed=ws, n_col=12, show=True,\n",
    "                    filepath = ''.join([struct.out_path,'pc_density_',category,'.png']))\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the first two PCs for the absolute velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_pcs(pc_feats, pc_labels,'abs_vel',pstruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's likely that the rapid changes in velocity due to bad tracking frames are affecting the PCA results. Not implemented yet, but will likely need to introduce filtering mechanisms that remove outliers from the PCA calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_pcs(pc_feats, pc_labels,'ego_euc',pstruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution across the first 2 PCs for egocentric euclidean positions appears to be be better, but we're still lacking the ability to resolve the behavioral space. We will need another method - preferably something that is non-linear and a bit more robust ... * *hint hint*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although velocities are calculated over rolling windows, the featurization we have so far still lacks the ability to capture complex temporal signals.\n",
    "\n",
    "To address this, we can leverage the frequency domain through a Morlet wavelet transformation.\n",
    "\n",
    "Let's see first what a Morlet wavelet looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "M = 100\n",
    "w0 = 3\n",
    "s = w0*90/(2*np.pi*25)\n",
    "morlet_wavelet = signal.morlet2(M, s, w0)\n",
    "plt.plot(morlet_wavelet.imag, label='Imaginary')\n",
    "plt.plot(morlet_wavelet.real, label='Real')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wavelets have the special property of being able to capture local frequency information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morlet wavelets have a real component and an imaginary component. When convolving with a time series, you get a complex time series. The magnitude corresponds to the spectral power. The angle from the real axis is the phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://mathvault.ca/wp-content/uploads/Euler-formula-diagram.png width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we calculate the power series for each PC across several frequencies of wavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlet_feats, wlet_labels = wavelet(pc_feats, \n",
    "                                  pc_labels, \n",
    "                                  pstruct.exp_ids_full,\n",
    "                                  sample_freq = 90,\n",
    "                                  freq = np.linspace(1,25,25),\n",
    "                                  w0 = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wlet_labels[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the spectrogram for the first principal component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(np.arange(180),np.linspace(1,25,25), wlet_feats[:180,:25].T, cmap='viridis', shading='gouraud')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Frequency')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save or read kinematic/wavelet features from h5 file\n",
    "save_h5(wlet_feats, wlet_labels, path = ''.join([pstruct.out_path,'kinematic_feats.h5']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use PCA to reduce the dimensions of the new wavelet features, and consolidate with previous PC scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_wlet, pc_wlet_labels = pca(wlet_feats,\n",
    "                              wlet_labels,\n",
    "                              categories = ['wlet_abs_vel','wlet_rel_vel','wlet_ego_euc','wlet_ang','wlet_avel'],\n",
    "                              n_pcs = 8,\n",
    "                              method = 'fbpca')\n",
    "del wlet_feats, wlet_labels\n",
    "pc_feats = np.hstack((pc_feats, pc_wlet))\n",
    "pc_labels += pc_wlet_labels\n",
    "del pc_wlet, pc_wlet_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save or read PCA-reduced features from h5 file\n",
    "save_h5(pc_feats, pc_labels, path = ''.join([pstruct.out_path,'pca_feats.h5']))\n",
    "# pc_feats, pc_labels = read_h5(path = ''.join([pstruct.out_path,'pca_feats.h5']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to run the rest of the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in analysis params\n",
    "params = itf.read_params_config(config_path = '../configs/fitsne.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DataStruct\n",
    "pstruct.features = pc_feats[::params['downsample']]\n",
    "pstruct.frame_id = np.arange(0,pc_feats.shape[0],params['downsample'])\n",
    "pstruct.exp_id = pstruct.exp_ids_full[::params['downsample']]\n",
    "pstruct.downsample = params['downsample']\n",
    "pstruct.load_meta()\n",
    "\n",
    "# Run the rest of the analysis\n",
    "pstruct = itf.run_analysis(params_config = '../configs/fitsne.yaml',\n",
    "                 ds = pstruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pstruct = pickle.load(open('/home/exx/Desktop/GitHub/results/R01_all/fitsne/datastruct.p','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-Distributed Stochastic Neighbor Embedding (t-SNE) is a non-linear 2D embedding and visualization algorithm. \n",
    "\n",
    "Through a stochastic process, t-SNE works by minimizing the KL divergence between a Gaussian distribution of distances in the high-D space and a Student's t-distribution of distances in the embedding space.\n",
    "\n",
    "The intuition is that points that are close together in high-D space should remain relatively close in the 2D map. And if clusters of data exist in the high-D space, they should be well represented in the low-D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.scatter(pstruct.embed_vals, show=True, \n",
    "            filepath=''.join([pstruct.out_path, 'final_scatter.png']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each point in this scatter plot represents 1 frame of video. You can see some large clusters as well some smaller clusters. These should represent different behavioral categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert the above plot into a density map, and segment out the behavior clusters using a watershed segmentation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.density(pstruct.ws.density, pstruct.ws.borders, show=True,\n",
    "            filepath = ''.join([pstruct.out_path,'final_density.png']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each border surrounds what we hope to be distinct behavioral categories. The entire map as a whole represents the behavioral expression of the animals in this dataset.\n",
    "\n",
    "Let's see how behavioral expression varies across conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.density_cat(data=pstruct, column='Condition', watershed=pstruct.ws, n_col=4, show=True,\n",
    "                filepath = ''.join([pstruct.out_path,'density_by_condition.png']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be a high amount of separation across conditions. Could be very likely that we are experiencing batch effects as some overlap should be expected. Will need further investigation.\n",
    "\n",
    "Let's look at the density per video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.density_cat(data=pstruct, column='exp_id', watershed=pstruct.ws, n_col=4, show=True,\n",
    "                filepath = ''.join([pstruct.out_path,'density_by_video.png']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes - there definitely appear to be some batch effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `plotly.express` to create interactive maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.imshow(pstruct.ws.watershed_map)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then sample and plot skeletons from various clusters of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.skeleton_vid3D_cat(pstruct,\n",
    "                       'Cluster',\n",
    "                       labels = [100,96,112,17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(pstruct.out_path + 'skeleton_vids/vis_Cluster_17.mp4', width=1200, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By loading back in our features, we can hope to classify clusters using some basic heuristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features back in\n",
    "features, labels =  read_h5(path = '/home/exx/Desktop/GitHub/results/R01_all/postural_feats.h5')\n",
    "features = features[::params['downsample'], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-Transformation\n",
    "features -= features.mean(axis=0)\n",
    "feat_std = np.std(features,axis=0)\n",
    "features = features[:,feat_std!=0] # Remove features with no variance\n",
    "features = features/feat_std[feat_std!=0]\n",
    "labels = [label for i, label in enumerate(labels) if feat_std[i]!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the expression of the head angular velocity across the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print([label for label in labels if 'avel' in label])\n",
    "\n",
    "# vis.density_feat(pstruct, pstruct.ws, features, labels, 'avel_0_3_4_xy_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'ego_euc_Snout_z'\n",
    "feat_key = features[:,labels.index(key)]\n",
    "f = plt.figure()\n",
    "extent = [*pstruct.ws.hist_range[0], *pstruct.ws.hist_range[1]]\n",
    "ax = f.add_subplot(111)\n",
    "# ax.plot(pstruct.ws.borders[:,0],pstruct.ws.borders[:,1],'.k',markersize=0.1)\n",
    "# ax.scatter(pstruct.ws.borders, zorder=1, extent=extent)\n",
    "ax.scatter(pstruct.embed_vals[::5,0], pstruct.embed_vals[::5,1],s=0.1,\n",
    "           c=feat_key[::5], cmap = 'viridis')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_aspect('auto')\n",
    "ax.set_xlabel('t-SNE 1')\n",
    "ax.set_ylabel('t-SNE 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also obtain the most differentially expressed features for different clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_feats_cluster(cluster,\n",
    "                          features,\n",
    "                          labels,\n",
    "                          pstruct):\n",
    "    feat_cluster = features[pstruct.data['Cluster'] == cluster,:]\n",
    "\n",
    "    feat_means = feat_cluster.mean(axis=0)\n",
    "    sorted_idx = np.argsort(feat_means)\n",
    "    sorted_feats = feat_means[sorted_idx]\n",
    "    sorted_labels = np.array(labels)[sorted_idx].tolist()\n",
    "\n",
    "    return sorted_labels[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feats = get_max_feats_cluster(97,\n",
    "                                  features,\n",
    "                                  labels,\n",
    "                                  pstruct)\n",
    "print(top_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be continued...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('capture')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "385c92b5077163337880ed1116fdafd989b4fe218c24e087321f17c8274ea8b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
